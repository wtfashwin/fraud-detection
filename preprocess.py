import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import joblib
import json
import os

DATA_PATH = 'data/creditcard.csv'
PROCESSED_DATA_PATH = 'data/preprocessed_data.npz'
MODELS_DIR = 'models'
os.makedirs(MODELS_DIR, exist_ok=True)

print("ğŸ“¥ Loading dataset...")
df = pd.read_csv(DATA_PATH)

print("ğŸ” Checking missing values:")
print(df.isnull().sum())

# Features and labels
X = df.drop('Class', axis=1)
y = df['Class']
feature_names = X.columns.tolist()  # Save before scaling

print("ğŸ”„ Scaling features...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("ğŸ”€ Splitting dataset (Train 80% / Test 20%)...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y.values, test_size=0.2, random_state=42, stratify=y
)

print("âš ï¸ Class balance before SMOTE â†’", np.bincount(y_train))

print("ğŸ” Applying SMOTE to balance training data...")
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_train, y_train)

print("âœ… Class balance after SMOTE â†’", np.bincount(y_res))

print("ğŸ’¾ Saving preprocessed arrays...")
np.savez_compressed(PROCESSED_DATA_PATH, X_res=X_res, y_res=y_res, X_test=X_test, y_test=y_test)

print("ğŸ’¾ Saving scaler...")
joblib.dump(scaler, os.path.join(MODELS_DIR, 'scaler.joblib'))

print("ğŸ’¾ Saving feature names...")
joblib.dump(feature_names, os.path.join(MODELS_DIR, 'columns.joblib'))
with open(os.path.join(MODELS_DIR, 'feature_names.json'), 'w') as f:
    json.dump(feature_names, f)

print("ğŸ Preprocessing complete.")
